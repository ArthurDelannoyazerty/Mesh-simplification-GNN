{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformation import Transformation\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from numpy import mean\n",
    "from igraph import Graph as igraphGraph\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.spatial.distance\n",
    "from scipy.spatial.distance import cdist\n",
    "from shapely.geometry import LineString, MultiLineString, Polygon, MultiPolygon, LinearRing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 5999\n",
      "Number of edges: 17991\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)  #  for repeatable results\n",
    "transformation = Transformation()\n",
    "\n",
    "user_number_triangles = 500   #à diminuer si le process est trop long\n",
    "number_neigh_tri = 20\n",
    "\n",
    "# Create objects\n",
    "stl_file_path = \"3d_models/stl/Handle.stl\"\n",
    "mesh_data = transformation.stl_to_mesh(stl_file_path)\n",
    "graph = transformation.mesh_to_graph(mesh_data)\n",
    "\n",
    "transformation.print_graph_properties(graph, display_graph=False, display_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(graph._node)<20:\n",
    "    raise Exception(\"Input mesh does not have enough vertices. (More than 20 is needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DevConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(array):\n",
    "    return np.maximum(array, 0)\n",
    "\n",
    "def sigmoid(array):\n",
    "    return 1 / (1 + np.exp(-array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahdel\\AppData\\Local\\Temp\\ipykernel_4884\\1471860285.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result_np = torch.stack([previous_inclusion_score, torch.tensor(list_inc_score)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5999])\n"
     ]
    }
   ],
   "source": [
    "class DevConv(nn.Module):\n",
    "    def __init__(self, nodes, adjacency_matrix, output_dimension):\n",
    "        super().__init__()\n",
    "        self.size = output_dimension\n",
    "        self.nodes = nodes\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "        self.W_phi = nn.Parameter(torch.Tensor(output_dimension))\n",
    "        self.W_theta = nn.Parameter(torch.Tensor(size=(3,1)))\n",
    "\n",
    "        nn.init.normal_(self.W_phi)\n",
    "        nn.init.normal_(self.W_theta)\n",
    "\n",
    "        # print(\"self.W_phi.shape : \", self.W_phi.shape)\n",
    "        # print(\"self.W_theta.shape : \", self.W_theta.shape)\n",
    "    \n",
    "    def forward(self, previous_inclusion_score, return_flatten=True):\n",
    "        list_inc_score = torch.zeros((self.nodes.shape[0], self.size))                                          #list of \"output_dimension\" for each \"list_node\" element\n",
    "        for index_current_node, list_neighbors in enumerate(self.adjacency_matrix):                             # for each node and its adjacency nodes\n",
    "            neighbors = self.nodes[list_neighbors.nonzero()]                                                    # get neighbors nodes\n",
    "            diff = self.nodes[index_current_node] - neighbors                                                   # Compute the differences between current_node and all neighbor nodes   (x_i - x_j)\n",
    "            to_norm = self.W_theta.T.unsqueeze(1).repeat(1, diff.shape[0], 1)[0] * diff.squeeze(1)              # Compute W_theta * (x_i - x_j)\n",
    "            neigh_distances = torch.norm(to_norm, dim=1)                                                        # Compute the norm for each vector difference  ||W_theta * (x_i - x_j)||\n",
    "            list_inc_score[index_current_node] = (self.W_phi * neigh_distances.max()).clone()                   # Add (W_phi * ||W_theta * (x_i - x_j)||) to the inclusion score list\n",
    "\n",
    "        if len(previous_inclusion_score)==0:                            # return if no previous inclusion score\n",
    "            if return_flatten:\n",
    "                list_inc_score = list_inc_score.flatten()\n",
    "            return list_inc_score\n",
    "        \n",
    "        if list_inc_score.shape[1]!=1:                                  # If inclusion score is not vector\n",
    "            list_inc_score = torch.mean(list_inc_score, dim=1)            # Mean the matrix for each node\n",
    "\n",
    "        # array of array to array\n",
    "        if len(list_inc_score.shape)==2:                 \n",
    "            if list_inc_score.shape[1]==1:\n",
    "                list_inc_score = list_inc_score.flatten()\n",
    "\n",
    "        result_np = torch.stack([previous_inclusion_score, torch.tensor(list_inc_score)])\n",
    "        \n",
    "        result_np = torch.mean(result_np, dim=0)\n",
    "        \n",
    "        return result_np\n",
    "\n",
    "\n",
    "class GNN_Model(nn.Module):\n",
    "    def __init__(self, nodes, adjacency_matrix):\n",
    "        super(GNN_Model, self).__init__()\n",
    "        self.devconv = DevConv(nodes, adjacency_matrix, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.devconv2 = DevConv(nodes, adjacency_matrix, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.devconv3 = DevConv(nodes, adjacency_matrix,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= self.devconv(x)\n",
    "        x= self.relu(x)\n",
    "        x= self.devconv2(x)\n",
    "        x= self.relu2(x)\n",
    "        x= self.devconv3(x)\n",
    "        x= self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "gnn = GNN_Model(torch.Tensor(np.array(graph)), torch.Tensor(nx.adjacency_matrix(graph).toarray()))\n",
    "inclusion_score = gnn(torch.empty(0))\n",
    "print(inclusion_score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1500, 3])\n"
     ]
    }
   ],
   "source": [
    "class MultinomialLayer(nn.Module):\n",
    "    def __init__(self, target_number_points,nodes):\n",
    "        super().__init__()\n",
    "        self.target_number_points = target_number_points\n",
    "        self.nodes = nodes\n",
    "\n",
    "    def forward(self, f):\n",
    "        normalized_inclusion_score = f / torch.sum(f)                           # normalize for multinomial sampling\n",
    "\n",
    "        mult_sampling = torch.distributions.multinomial.Multinomial(total_count=10*normalized_inclusion_score.shape[0], probs=normalized_inclusion_score).sample()      # small:more randomness    |   big:less randomness\n",
    "        mult_indices = mult_sampling.topk(k=self.target_number_points).indices\n",
    "        selected_nodes = self.nodes[mult_indices]\n",
    "\n",
    "        return selected_nodes\n",
    "\n",
    "\n",
    "target_number_point = min(len(graph._node), user_number_triangles*3)   # number of points for the simplification\n",
    "layer = MultinomialLayer(target_number_point, torch.Tensor(np.array(graph)))\n",
    "extended_graph_nodes = layer.forward(torch.Tensor(inclusion_score))\n",
    "print(extended_graph_nodes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN extended graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1500\n",
      "Number of edges: 12612\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import pairwise_distance\n",
    "\n",
    "def create_knn_graph(nodes, k):\n",
    "    \"\"\"\n",
    "    Create a graph based on k-nearest neighbors using PyTorch.\n",
    "    Parameters:\n",
    "    - nodes: Tensor of shape (n, 3) representing 3D nodes.\n",
    "    - k: Number of nearest neighbors.\n",
    "    Returns:\n",
    "    - adjacency_matrix: Binary adjacency matrix representing the graph.\n",
    "    \"\"\"\n",
    "    expanded_x1 = nodes.unsqueeze(1)\n",
    "    expanded_x2 = nodes.unsqueeze(0)\n",
    "    distances = torch.norm(expanded_x1 - expanded_x2, dim=2)        # distance matrix\n",
    "\n",
    "    _, indices = torch.topk(distances, k + 1, largest=False, sorted=True, dim=1)\n",
    "    indices = indices[:, 1:]  # Exclude the node itself\n",
    "\n",
    "    # Create adjacency matrix\n",
    "    adjacency_matrix = torch.zeros(nodes.shape[0], nodes.shape[0], dtype=torch.float32)\n",
    "    adjacency_matrix.scatter_(1, indices, 1)\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "extended_graph_adjacency_matrix = create_knn_graph(extended_graph_nodes, k=15)\n",
    "transformation.print_graph_properties(graph=nx.from_numpy_array(extended_graph_adjacency_matrix.numpy()), display_graph=False, display_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1500, 3])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_graph_nodes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1500, 64])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devconv = DevConv(extended_graph_nodes,extended_graph_adjacency_matrix, 64)\n",
    "inclusion_score = devconv(previous_inclusion_score=torch.empty((0)), return_flatten=False)\n",
    "inclusion_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1500, 1500])\n"
     ]
    }
   ],
   "source": [
    "class SparseAttentionEdgePredictorLayer(nn.Module):\n",
    "    def __init__(self, nodes, neighbors, size=64):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.nodes = nodes\n",
    "        self.neighbors = neighbors\n",
    "        self.wq = nn.Parameter(torch.Tensor(size))\n",
    "        self.wk = nn.Parameter(torch.Tensor(size))\n",
    "\n",
    "        nn.init.normal_(self.wq)\n",
    "        nn.init.normal_(self.wk)\n",
    "\n",
    "    def forward(self, f):\n",
    "        wq_f = self.wq.reshape(-1, 1) * f                   # Wq*f\n",
    "        wk_f = self.wk.reshape(-1, 1) * f                   # Wq*f\n",
    "        S = torch.exp(torch.matmul(wq_f.T, wk_f))           # e^((wq_f.T)*(wk_f))\n",
    "        \n",
    "        nonzero_neigh = self.neighbors.nonzero()                                                    # Find indexes of neighbors in graph\n",
    "        unique_first_elements, counts = torch.unique(nonzero_neigh[:, 0], return_counts=True)       # Count number of neighbors per node\n",
    "        split_tensors = list(torch.split(nonzero_neigh, tuple(counts)))                             # split indexes of neighbors into a list (1 element = 1 tensor of indexes)\n",
    "\n",
    "        temp = [[S[n[i,0], n[i,1]] for i in range(len(n))] for n in split_tensors]                  # For each node, get the S value for the neighbors indexes\n",
    "        summed = torch.Tensor([torch.sum(torch.Tensor(e)) for e in temp])                           # Sum these results for each nodes\n",
    "        division = summed.unsqueeze(0).repeat(1, S.shape[1], 1)[0]                                  # Repeat the sum in S.shape[1] array => division per columns\n",
    "        final_term  = S / division\n",
    "\n",
    "        return final_term\n",
    "\n",
    "\n",
    "f = torch.mean(inclusion_score, dim=1)                            # Flatten the matrix of inclusion score\n",
    "layer = SparseAttentionEdgePredictorLayer(extended_graph_nodes, extended_graph_adjacency_matrix)\n",
    "S = layer.forward(torch.Tensor(f))\n",
    "print(S.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = S*np.random.choice([0, 1], size=S.shape)      # Add a random mask to emulate the 'sparse'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1500, 1500])\n"
     ]
    }
   ],
   "source": [
    "class FaceCandidatesLayer(nn.Module):\n",
    "    def __init__(self, adjacency_matrix):\n",
    "        super().__init__()\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "\n",
    "    def forward(self, S):\n",
    "        A_s = torch.matmul(torch.matmul(S, self.adjacency_matrix), S.T)     # A_s = S * A * S.T\n",
    "        A_s = A_s/A_s.max()                                                 # Normalize\n",
    "        return A_s\n",
    "\n",
    "\n",
    "layer = FaceCandidatesLayer(extended_graph_adjacency_matrix)\n",
    "A_s = layer(torch.Tensor(S))\n",
    "print(A_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TriConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGRAPH U-W- 1500 12612 -- \n",
      "+ attr: _nx_name (v), weight (e)\n"
     ]
    }
   ],
   "source": [
    "igraph_g = igraphGraph(directed=False).from_networkx(nx.from_numpy_array(extended_graph_adjacency_matrix.numpy()))\n",
    "print(igraph_g.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43747, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "triangles_ids_igraph = np.array(igraph_g.cliques(min=3, max=3))\n",
    "triangles = extended_graph_nodes[triangles_ids_igraph]\n",
    "print(triangles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero = extended_graph_adjacency_matrix.nonzero()\n",
    "# print(nonzero.shape)\n",
    "neighbors_one_indexes = nonzero.reshape(1500,15,2)[:,:,1].clone()\n",
    "# print(\"neighbors_indexes : \", neighbors_one_indexes.shape, neighbors_one_indexes)\n",
    "\n",
    "neighbors_two_indexes = neighbors_one_indexes[neighbors_one_indexes]\n",
    "# neighbors_two_indexes = neighbors_two_indexes[:,:,1:]                                                   # remove returns from the one neighbors\n",
    "# print(\"neighbors_two_indexes : \", neighbors_two_indexes.shape, neighbors_two_indexes)\n",
    "\n",
    "neighbors_three_indexes = neighbors_one_indexes[neighbors_two_indexes]\n",
    "# print(\"neighbors_three_indexes : \", neighbors_three_indexes.shape, neighbors_three_indexes)\n",
    "\n",
    "# Find the indices where the current index is present along the last dimension\n",
    "values_index_reshape = torch.arange(neighbors_three_indexes.shape[0]).repeat((15,15,15,1)).T\n",
    "# print(\"temp : \", values_index_reshape.shape)\n",
    "indices = (neighbors_three_indexes == values_index_reshape).nonzero()\n",
    "# print(indices.shape)\n",
    "# print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j, k, l = indices[:,0], indices[:,1], indices[:,2], indices[:,3]\n",
    "temp_j = neighbors_one_indexes[i,j]\n",
    "temp_k = neighbors_two_indexes[i,j,k]\n",
    "temp_l = neighbors_three_indexes[i,j,k,l]\n",
    "triangles_indexes_test = torch.stack((i, temp_j, temp_k, temp_l), dim=1)\n",
    "triangles_indexes_test = triangles_indexes_test[:,:3]               # remove virtual 4th point\n",
    "print(triangles_indexes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter triangles indexes to clean the clones (=> divide the number of triangles by 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196926, 3, 3])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_graph_nodes[triangles_indexes_test].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43747])\n"
     ]
    }
   ],
   "source": [
    "class FirstPInitLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, triangles_indexes):\n",
    "        # Extract indices for each triangle\n",
    "        i, j, k = triangles_ids_igraph.T\n",
    "\n",
    "        # Extract probabilities using advanced indexing\n",
    "        A_s_ij = A_s[i, j]\n",
    "        A_s_ik = A_s[i, k]\n",
    "        A_s_jk = A_s[j, k]\n",
    "\n",
    "        # Calculate the barycenter probabilities\n",
    "        p_init = torch.zeros(len(triangles))\n",
    "        p_init = (A_s_ij + A_s_ik + A_s_jk) / 3\n",
    "        return p_init\n",
    "\n",
    "p_init_layer = FirstPInitLayer()\n",
    "p_init = p_init_layer(triangles_ids_igraph)\n",
    "print(p_init.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate barycenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43747, 3])\n"
     ]
    }
   ],
   "source": [
    "class BarycentersLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, triangles):\n",
    "        return triangles.mean(1)\n",
    "\n",
    "barycenters_layer = BarycentersLayer()\n",
    "barycenters = barycenters_layer(triangles)\n",
    "print(barycenters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43747, 20])\n"
     ]
    }
   ],
   "source": [
    "class KNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, k=20):\n",
    "        indices_knn = torch.empty(size=(x.shape[0], k))\n",
    "        \n",
    "        for i, neighbors in enumerate(x):\n",
    "            distances = torch.norm(neighbors-x, dim=1)\n",
    "            # print(distances.shape)\n",
    "            # print(distances)\n",
    "            # print(i)\n",
    "\n",
    "            indices_knn[i] = distances.topk(k, dim=0, largest=False).indices.clone()  # Indices of the k-nearest neighbors\n",
    "            # print(indices_knn.shape)\n",
    "            # print(indices_knn)\n",
    "            \n",
    "        return indices_knn\n",
    "\n",
    "knn_layer = KNN()\n",
    "indices_neigh_tri = knn_layer(barycenters).int()  #change datatype\n",
    "print(indices_neigh_tri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([43747, 19, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RMatrix(nn.Module):\n",
    "    def __init__(self, triangles, barycenters, indices_neigh_tri):\n",
    "        super().__init__()\n",
    "        self.triangles = triangles\n",
    "        self.barycenters = barycenters\n",
    "        self.indices_neigh_tri = indices_neigh_tri\n",
    "\n",
    "    def forward(self):\n",
    "        # DIFF BARYCENTERS\n",
    "        barycenters_diff = np.subtract(self.barycenters[self.indices_neigh_tri[:, 0]][:, np.newaxis], self.barycenters[self.indices_neigh_tri[:, 1:]])   #Inverser la différence des barycentres si nécéssaire\n",
    "\n",
    "\n",
    "        # TRIANGLE EDGES NORM\n",
    "        v0, v1, v2 = self.triangles[:, 0], self.triangles[:, 1], self.triangles[:, 2]\n",
    "\n",
    "        # Calculate edge vectors\n",
    "        e_ij = torch.norm(v0 - v1, dim=1)\n",
    "        e_ik = torch.norm(v0 - v2, dim=1)\n",
    "        e_jk = torch.norm(v1 - v2, dim=1)\n",
    "\n",
    "        # Stack the edge vectors along the last dimension\n",
    "        diff_vectors = torch.stack([e_ij, e_ik, e_jk], dim=1)\n",
    "\n",
    "\n",
    "        # MAX/MIN DIFF VECTORS\n",
    "        max_diff_vectors = diff_vectors.max(dim=1).values       # calculate t_n_max\n",
    "        min_diff_vectors = diff_vectors.min(dim=1).values       # calculate t_n_min\n",
    "\n",
    "        max_diff_vectors_diff = max_diff_vectors[self.indices_neigh_tri[:, 0]][:, None] - max_diff_vectors[self.indices_neigh_tri[:, 1:]]   #Inverser la différence des barycentres si nécéssaire   # calculate t_n_max - t_m_max\n",
    "        min_diff_vectors_diff = min_diff_vectors[self.indices_neigh_tri[:, 0]][:, None] - min_diff_vectors[self.indices_neigh_tri[:, 1:]]   #Inverser la différence des barycentres si nécéssaire   # calculate t_n_min - t_m_min\n",
    "\n",
    "\n",
    "        # R MATRIX COMPUTATION\n",
    "        r_matrix = torch.zeros((self.triangles.shape[0], number_neigh_tri-1, 5))\n",
    "\n",
    "        r_matrix[:, :, 0]   = min_diff_vectors_diff\n",
    "        r_matrix[:, :, 1]   = max_diff_vectors_diff\n",
    "        r_matrix[:, :, 2:5] = barycenters_diff\n",
    "        \n",
    "        return r_matrix\n",
    "\n",
    "\n",
    "r_matrix_layer = RMatrix(triangles, barycenters, indices_neigh_tri)\n",
    "r_matrix = r_matrix_layer()\n",
    "r_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, r_matrix):\n",
    "    super().__init__()\n",
    "    self.r_matrix = r_matrix\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(32 * 32 * 3, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    return self.layers(x)\n",
    "\n",
    "f_init = p_init\n",
    "mlp = MLP(r_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99999964\n",
      "(43747,)\n",
      "[2.3804538e-05 2.3797165e-05 2.3302142e-05 ... 2.1143813e-05 2.1072487e-05\n",
      " 2.1078808e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_6276\\474768578.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  final_scores = torch.nn.functional.softmax(torch.tensor(f_final))    #proba des triangles\n",
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_6276\\474768578.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_scores = torch.nn.functional.softmax(torch.tensor(f_final))    #proba des triangles\n"
     ]
    }
   ],
   "source": [
    "# Multi Layer Perceptron (MLP) * 3 \n",
    "f_final = p_init    # TODO\n",
    "\n",
    "\n",
    "final_scores = torch.nn.functional.softmax(torch.tensor(f_final))    #proba des triangles\n",
    "final_scores = final_scores.numpy()\n",
    "print(final_scores.sum())\n",
    "print(final_scores.shape)\n",
    "print(final_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 3, 3)\n",
      "[[ 51.         -3.0965528 -13.237194 ]\n",
      " [ 39.         -8.699333   -3.6727462]\n",
      " [ 51.          4.590885    2.507581 ]]\n"
     ]
    }
   ],
   "source": [
    "selected_triangles_indexes = np.argpartition(final_scores, -int(target_number_point/3))[-int(target_number_point/3):] \n",
    "selected_triangles = np.array(triangles)[selected_triangles_indexes]\n",
    "print(selected_triangles.shape) # number triangles, number points, number dimensions(x,y,z)\n",
    "print(selected_triangles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 43\n",
      "Number of edges: 203\n"
     ]
    }
   ],
   "source": [
    "simplified_final_graph = nx.Graph()\n",
    "for index_poly, poly in enumerate(selected_triangles):\n",
    "    for index_current_node in range(len(poly)):\n",
    "        current_node = tuple(poly[index_current_node])\n",
    "        for index_other_node in range(index_current_node+1, len(poly)):\n",
    "            edge = current_node, tuple(poly[index_other_node])\n",
    "            simplified_final_graph.add_edge(*edge)\n",
    "            # if attribute do not exists\n",
    "            if len(simplified_final_graph.nodes[current_node])==0:\n",
    "                simplified_final_graph.nodes[current_node]['index_triangle'] = set()\n",
    "            simplified_final_graph.nodes[current_node]['index_triangle'].add(index_poly)\n",
    "            if len(simplified_final_graph.nodes[tuple(poly[index_other_node])])==0:\n",
    "                simplified_final_graph.nodes[tuple(poly[index_other_node])]['index_triangle'] = set()\n",
    "            simplified_final_graph.nodes[tuple(poly[index_other_node])]['index_triangle'].add(index_poly)\n",
    "            \n",
    "transformation.print_graph_properties(graph=simplified_final_graph, display_graph=False, display_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_final_mesh = transformation.graph_to_mesh(simplified_final_graph)\n",
    "\n",
    "#Affichage\n",
    "transformation.mesh_to_display_vtk(mesh_data)\n",
    "transformation.mesh_to_display_vtk(simplified_final_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Chamfer distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_d_P_Ps(p_y, x, y):\n",
    "    \"\"\"All Tensors in input\"\"\"\n",
    "    # print(p_y.shape, x.shape, y.shape)\n",
    "\n",
    "    expanded_x1 = x.unsqueeze(1)\n",
    "    expanded_x2 = y.unsqueeze(0)\n",
    "    distances = torch.norm(expanded_x1 - expanded_x2, dim=2)        # distance matrix\n",
    "\n",
    "    min_x = distances.min(dim=1).values\n",
    "    min_y = distances.min(dim=0)\n",
    "\n",
    "    first_term = torch.sum(torch.index_select(p_y, 0, min_y.indices) * min_y.values)\n",
    "    second_term = torch.sum(min_x * p_y)\n",
    "\n",
    "    return first_term + second_term\n",
    "\n",
    "\n",
    "d_P_Ps = torch_d_P_Ps(torch.Tensor(normalized_inclusion_score), torch.Tensor(np.array(graph)), torch.Tensor(np.array(extended_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Surfaces Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d_f_S_Ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_d_f_S_Ss(p_b_hat, b_hat, b):\n",
    "    # print(p_b_hat.shape, b_hat.shape, b.shape)\n",
    "\n",
    "    expanded_x1 = b_hat.unsqueeze(1)\n",
    "    expanded_x2 = b.unsqueeze(0)\n",
    "    distances = torch.norm(expanded_x1 - expanded_x2, dim=2)\n",
    "\n",
    "    min_b = distances.min(dim=1).values\n",
    "\n",
    "    final_term = torch.sum(p_b_hat * min_b)\n",
    "\n",
    "    return final_term\n",
    "\n",
    "\n",
    "\n",
    "igraph_g_original = igraphGraph(directed=extended_graph.is_directed()).from_networkx(graph)\n",
    "triangles_ids_igraph_original = np.array(igraph_g_original.cliques(min=3, max=3))\n",
    "triangles = np.array(igraph_g_original.vs['_nx_name'])[triangles_ids_igraph_original]\n",
    "torch_b = torch.Tensor(np.mean(triangles, axis=1))\n",
    "\n",
    "torch_b_hat = torch.Tensor(np.mean(selected_triangles, axis=1))\n",
    "\n",
    "torch_p_b_hat = torch.Tensor(final_scores[selected_triangles_indexes])\n",
    "\n",
    "d_f_S_Ss = torch_d_f_S_Ss(torch_p_b_hat, torch_b_hat, torch_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d_r_S_Ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0768)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def torch_d_r_S_Ss(p_x, p_y, x ,y):\n",
    "    expanded_x = x.unsqueeze(1)\n",
    "    expanded_y = y.unsqueeze(0)\n",
    "    distances = torch.norm(expanded_x - expanded_y, dim=2)\n",
    "    min_d = distances.min(dim=0).values\n",
    "    first_term = p_y * min_d\n",
    "\n",
    "\n",
    "    indices_knn = distances.topk(k=50, dim=0, largest=False).indices.T  # Indices of the k-nearest neighbors\n",
    "    knn_labels = x[indices_knn]\n",
    "    xtk = torch.reshape(knn_labels, shape=((y.shape[0])*50, 3))\n",
    "\n",
    "    expanded_xtk = xtk.unsqueeze(1)\n",
    "    distances_knn = torch.norm(expanded_xtk - expanded_y, dim=2)\n",
    "    min_knn = distances_knn.min(dim=1).values\n",
    "    min_knn_reshaped = min_knn.reshape(((y.shape[0]), 50))\n",
    "\n",
    "    ptk_time_norm = p_x[indices_knn] * min_knn_reshaped\n",
    "    factor = (1-p_y) * (1/50)\n",
    "    second_term = factor * torch.sum(ptk_time_norm, dim=1)\n",
    "\n",
    "    final_term = torch.sum(first_term + second_term)\n",
    "\n",
    "    return final_term\n",
    "\n",
    "d_f_S_Ss = torch_d_r_S_Ss(torch.Tensor(final_scores), torch_p_b_hat, torch_b, torch_b_hat)\n",
    "d_f_S_Ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangle Collision Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lc_le_lo(p_t, m_c_e_o, Fs):\n",
    "    \"\"\"\n",
    "    Compute the collision loss term L_c.\n",
    "\n",
    "    Parameters:\n",
    "    - p_t: 1D numpy array containing the probabilities of each triangle (indices)\n",
    "    - m_c_t: 2D numpy array containing the number of faces penetrated by each triangle\n",
    "    - Fs: 3D numpy array representing the vertices of triangles\n",
    "\n",
    "    Returns:\n",
    "    - L_c: Collision loss term\n",
    "    \"\"\"\n",
    "    assert len(p_t) == len(m_c_e_o), \"Input arrays must have the same length\"\n",
    "\n",
    "    penalty_per_triangle = p_t * m_c_e_o\n",
    "\n",
    "    # Sum the penalties for all selected triangles\n",
    "    total_penalty = np.sum(penalty_per_triangle)\n",
    "\n",
    "    # Compute the collision loss term L_c\n",
    "    L_c_e_o = (1 / len(Fs)) * total_penalty\n",
    "\n",
    "    return L_c_e_o\n",
    "\n",
    "# Example usage:\n",
    "# Replace the arrays below with your actual data\n",
    "# p_t = selected_triangles_indexes\n",
    "# m_c_t = numpy array containing the number of faces penetrated by each triangle\n",
    "# Fs = 3D numpy array representing the vertices of triangles\n",
    "p_t = selected_triangles_indexes\n",
    "Fs = triangles  # Given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b_hat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m number_neigh_barycenters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mb_hat\u001b[49m))\n\u001b[0;32m      2\u001b[0m _, indexes_neigh_selected_barycenters \u001b[38;5;241m=\u001b[39m NearestNeighbors(n_neighbors\u001b[38;5;241m=\u001b[39mnumber_neigh_barycenters)\u001b[38;5;241m.\u001b[39mfit(b_hat)\u001b[38;5;241m.\u001b[39mkneighbors(b_hat)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'b_hat' is not defined"
     ]
    }
   ],
   "source": [
    "number_neigh_barycenters = min(50, len(b_hat))\n",
    "_, indexes_neigh_selected_barycenters = NearestNeighbors(n_neighbors=number_neigh_barycenters).fit(b_hat).kneighbors(b_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_c :  4225.18776054694\n"
     ]
    }
   ],
   "source": [
    "mc = np.zeros((500))\n",
    "\n",
    "for index_neigh_barycenters in indexes_neigh_selected_barycenters:\n",
    "    current_triangle = selected_triangles[index_neigh_barycenters[0]]\n",
    "    others_triangles = selected_triangles[index_neigh_barycenters[1:]]\n",
    "\n",
    "    lines_current_triangle = LinearRing(current_triangle)\n",
    "    polygons_others_tri = MultiPolygon([Polygon(others_triangle) for others_triangle in others_triangles]).buffer(0)    # buffer 0 to correct invalid polygons => take the exterior of the shape\n",
    "\n",
    "\n",
    "    intersection = lines_current_triangle.intersection(polygons_others_tri)\n",
    "    if intersection.is_empty:\n",
    "        continue\n",
    "    if intersection.geom_type == 'MultiLineString':\n",
    "        mc[index_neigh_barycenters[0]] = len(intersection.geoms)\n",
    "    else:\n",
    "        mc[index_neigh_barycenters[0]] += 1\n",
    "L_c = compute_lc_le_lo(p_t, mc, Fs)\n",
    "print(\"L_c : \", L_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_e :  24105.658829414708\n"
     ]
    }
   ],
   "source": [
    "me = np.zeros((500))\n",
    "\n",
    "for index_neigh_barycenters in indexes_neigh_selected_barycenters:\n",
    "    current_triangle = selected_triangles[index_neigh_barycenters[0]]\n",
    "    others_triangles = selected_triangles[index_neigh_barycenters[1:]]\n",
    "\n",
    "    lines_current_triangle = LinearRing(current_triangle)\n",
    "    lines_others_tri = MultiLineString([LineString([other_tri[0], other_tri[1], other_tri[2], other_tri[0]]) for other_tri in others_triangles])\n",
    "\n",
    "    intersection = lines_current_triangle.intersection(lines_others_tri)\n",
    "    if intersection.is_empty:\n",
    "        continue\n",
    "    if intersection.geom_type == 'MultiLineString':\n",
    "        me[index_neigh_barycenters[0]] = len(intersection.geoms)\n",
    "    else:\n",
    "        me[index_neigh_barycenters[0]] += 1\n",
    "L_e = compute_lc_le_lo(p_t, me, Fs)\n",
    "print(\"L_e : \", L_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = np.zeros((500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo - échantillonnage de 100 points à partir de chaque triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def sample_points_from_triangle(t, num_points=100):\n",
    "    v1, v2, v3 = t\n",
    "    bary_coords = np.random.rand(num_points, 2)\n",
    "    sqrt_bary_coords = np.sqrt(bary_coords[:, 0])\n",
    "\n",
    "    u = sqrt_bary_coords\n",
    "    v = bary_coords[:, 1]\n",
    "\n",
    "    \"\"\"\n",
    "    La formule spécifique est dérivée de l'expression générale d'interpolation barycentrique \n",
    "    sommets A, B et C\n",
    "    coord barycentriques: u et v\n",
    "    coord cartésiennes: x,y,z \n",
    "    \"\"\"\n",
    "    x_coords = (1 - u - v) * v1[0] + u * v2[0] + v * v3[0]\n",
    "    y_coords = (1 - u - v) * v1[1] + u * v2[1] + v * v3[1]\n",
    "    z_coords = (1 - u - v) * v1[2] + u * v2[2] + v * v3[2]\n",
    "\n",
    "    sampled_points = np.column_stack((x_coords, y_coords, z_coords))\n",
    "    return sampled_points\n",
    "\n",
    "points100 = sample_points_from_triangle(triangle, num_points=100)\n",
    "points100\n",
    "points100.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les Aires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_neigh_selected_barycenters = min(50, len(b_hat))\n",
    "def knnbar(nn):\n",
    "  _, indexes_neigh_selected_barycenters = NearestNeighbors(n_neighbors=nn).fit(b_hat).kneighbors(b_hat)\n",
    "  return _, indexes_neigh_selected_barycenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 292,  20, ..., 181, 180, 141],\n",
       "       [  1,   6, 344, ..., 397, 151, 473],\n",
       "       [  2, 186,  10, ..., 185, 184, 189],\n",
       "       ...,\n",
       "       [497, 421, 418, ..., 379, 238, 457],\n",
       "       [498, 261, 303, ..., 385,  40, 327],\n",
       "       [499, 264, 126, ..., 404, 338, 320]], dtype=int64)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, indices = knnbar(number_neigh_selected_barycenters)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pénalité pour les triangles qui se chevauchent : 0.0008337502084375521\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance as scipy_distance\n",
    "\n",
    "def calculate_triangle_area(triangle):\n",
    "    # Fonction pour calculer l'aire d'un triangle en utilisant la formule de Héron\n",
    "    side_lengths = [scipy_distance.euclidean(triangle[i], triangle[(i + 1) % 3]) for i in range(3)]\n",
    "    s = sum(side_lengths) / 2\n",
    "    return np.sqrt(s * np.prod([s - length for length in side_lengths]))\n",
    "\n",
    "def penalize_overlapping_triangles(points_list, triangles, k=50):\n",
    "    # Fonction pour pénaliser les triangles qui se chevauchent\n",
    "    assignment_results = []\n",
    "    overlapping_triangles = np.zeros(len(triangles), dtype=int)  # Déclaration en dehors de la boucle\n",
    "\n",
    "    # ajustement de la valeur maximale de k en fonction du nombre de triangles\n",
    "    k = min(k, len(triangles))\n",
    "\n",
    "    # NearestNeighbors pour trouver les k triangles les plus proches pour chaque point\n",
    "    knn_model = NearestNeighbors(n_neighbors=k).fit(np.vstack(triangles))\n",
    "    d, indices = knn_model.kneighbors(np.array(points_list)[:, 0, :])\n",
    "\n",
    "    for i, point_list in enumerate(points_list):\n",
    "        point = point_list[0]\n",
    "\n",
    "        # Verif que les indices sont valides\n",
    "        closest_triangle_indices = indices[i, :k]\n",
    "        closest_triangle_indices = closest_triangle_indices[closest_triangle_indices < len(triangles)]\n",
    "\n",
    "        # Accéder aux triangles en utilisant les indices valides\n",
    "        valid_triangle_indices = []\n",
    "        for idx in closest_triangle_indices:\n",
    "            modified_triangle = np.copy(triangles[idx])\n",
    "            modified_triangle[0] = point\n",
    "\n",
    "            area_original = calculate_triangle_area(triangles[idx])\n",
    "            area_modified = calculate_triangle_area(modified_triangle)\n",
    "\n",
    "            # Vérifier si la somme des aires est proche de l'aire du triangle\n",
    "            if not np.isclose(area_original, area_modified, rtol=1e-5):\n",
    "                assignment_results.append((point, triangles[idx]))\n",
    "                valid_triangle_indices.append(idx)\n",
    "\n",
    "        # MAJ du overlapping_triangles avec les indices valides\n",
    "        overlapping_triangles[valid_triangle_indices] += 1\n",
    "\n",
    "    penalties = overlapping_triangles\n",
    "    total_penalty = np.sum(penalties)\n",
    "    Lo = (1 / len(triangles)) * total_penalty\n",
    "\n",
    "    return Lo\n",
    "\n",
    "\n",
    "L_o = penalize_overlapping_triangles([points100], triangles, k=50)\n",
    "print(\"Pénalité pour les triangles qui se chevauchent :\", L_o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambda_c, lambda_e, lambda_o = 0.01, 0.01, 0.01\n",
    "\n",
    "\n",
    "Loss_L = prob_chamfer_dist + d_f_S_Ss + d_r_S_Ss + (lambda_c * L_c) + (lambda_e * L_e) + (lambda_o * L_o)\n",
    "print(Loss_L)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meshPFE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
