{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformation import Transformation\n",
    "import numpy as np\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from igraph import Graph as igraphGraph\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "# from scipy.spatial.distance import cdist\n",
    "# from shapely.geometry import LineString, MultiLineString, Polygon, MultiPolygon, LinearRing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)  #  for repeatable results\n",
    "# transformation = Transformation()\n",
    "\n",
    "# user_number_triangles = 500   #Ã  diminuer si le process est trop long\n",
    "# number_neigh_tri = 20\n",
    "\n",
    "# # Create objects\n",
    "# stl_file_path = \"3d_models/stl/Handle.stl\"\n",
    "# mesh_data = transformation.stl_to_mesh(stl_file_path)\n",
    "# graph = transformation.mesh_to_graph(mesh_data)\n",
    "\n",
    "# transformation.print_graph_properties(graph, display_graph=False, display_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(graph._node)<20:\n",
    "#     raise Exception(\"Input mesh does not have enough vertices. (More than 20 is needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DevConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def relu(array):\n",
    "#     return np.maximum(array, 0)\n",
    "\n",
    "# def sigmoid(array):\n",
    "#     return 1 / (1 + np.exp(-array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_nodes = torch.Tensor(np.array(graph))\n",
    "# graph_adjacency_matrix = torch.Tensor(nx.adjacency_matrix(graph).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevConv(nn.Module):\n",
    "    def __init__(self, nodes, adjacency_matrix, output_dimension):\n",
    "        super().__init__()\n",
    "        self.size = output_dimension\n",
    "        self.nodes = nodes\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "        self.W_phi = nn.Parameter(torch.Tensor(output_dimension))\n",
    "        self.W_theta = nn.Parameter(torch.Tensor(size=(3,1)))\n",
    "\n",
    "        nn.init.normal_(self.W_phi)\n",
    "        nn.init.normal_(self.W_theta)\n",
    "\n",
    "        # print(\"self.W_phi.shape : \", self.W_phi.shape)\n",
    "        # print(\"self.W_theta.shape : \", self.W_theta.shape)\n",
    "    \n",
    "    def forward(self, previous_inclusion_score, return_flatten=True):\n",
    "        list_inc_score = torch.zeros((self.nodes.shape[0], self.size))                                          #list of \"output_dimension\" for each \"list_node\" element\n",
    "        for index_current_node, list_neighbors in enumerate(self.adjacency_matrix):                             # for each node and its adjacency nodes\n",
    "            neighbors = self.nodes[list_neighbors.nonzero()]                                                    # get neighbors nodes\n",
    "            diff = self.nodes[index_current_node] - neighbors                                                   # Compute the differences between current_node and all neighbor nodes   (x_i - x_j)\n",
    "            to_norm = self.W_theta.T.unsqueeze(1).repeat(1, diff.shape[0], 1)[0] * diff.squeeze(1)              # Compute W_theta * (x_i - x_j)\n",
    "            neigh_distances = torch.norm(to_norm, dim=1)                                                        # Compute the norm for each vector difference  ||W_theta * (x_i - x_j)||\n",
    "            list_inc_score[index_current_node] = (self.W_phi * neigh_distances.max()).clone()                   # Add (W_phi * ||W_theta * (x_i - x_j)||) to the inclusion score list\n",
    "\n",
    "        if len(previous_inclusion_score)==0:                            # return if no previous inclusion score\n",
    "            if return_flatten:\n",
    "                list_inc_score = list_inc_score.flatten()\n",
    "            return list_inc_score\n",
    "        \n",
    "        if list_inc_score.shape[1]!=1:                                  # If inclusion score is not vector\n",
    "            list_inc_score = torch.mean(list_inc_score, dim=1)            # Mean the matrix for each node\n",
    "\n",
    "        # array of array to array\n",
    "        if len(list_inc_score.shape)==2:                 \n",
    "            if list_inc_score.shape[1]==1:\n",
    "                list_inc_score = list_inc_score.flatten()\n",
    "\n",
    "        result_np = torch.stack([previous_inclusion_score, torch.tensor(list_inc_score)])\n",
    "        \n",
    "        result_np = torch.mean(result_np, dim=0)\n",
    "        \n",
    "        return result_np\n",
    "\n",
    "\n",
    "class GNN_Model(nn.Module):\n",
    "    def __init__(self, nodes, adjacency_matrix):\n",
    "        super(GNN_Model, self).__init__()\n",
    "        self.devconv = DevConv(nodes, adjacency_matrix, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.devconv2 = DevConv(nodes, adjacency_matrix, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.devconv3 = DevConv(nodes, adjacency_matrix,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= self.devconv(x)\n",
    "        x= self.relu(x)\n",
    "        x= self.devconv2(x)\n",
    "        x= self.relu2(x)\n",
    "        x= self.devconv3(x)\n",
    "        x= self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# gnn = GNN_Model(graph_nodes, graph_adjacency_matrix)\n",
    "# inclusion_score = gnn(torch.empty(0))\n",
    "# print(inclusion_score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialLayer(nn.Module):\n",
    "    def __init__(self, target_number_points,nodes):\n",
    "        super().__init__()\n",
    "        self.target_number_points = target_number_points\n",
    "        self.nodes = nodes\n",
    "\n",
    "    def forward(self, f):\n",
    "        normalized_inclusion_score = f / torch.sum(f)                           # normalize for multinomial sampling\n",
    "\n",
    "        mult_sampling = torch.distributions.multinomial.Multinomial(total_count=10*normalized_inclusion_score.shape[0], probs=normalized_inclusion_score).sample()      # small:more randomness    |   big:less randomness\n",
    "        mult_indices = mult_sampling.topk(k=self.target_number_points).indices\n",
    "        selected_nodes = self.nodes[mult_indices]\n",
    "\n",
    "        return selected_nodes\n",
    "\n",
    "\n",
    "# target_number_point = min(len(graph._node), user_number_triangles*3)   # number of points for the simplification\n",
    "# layer = MultinomialLayer(target_number_point, graph_nodes)\n",
    "# extended_graph_nodes = layer.forward(inclusion_score)\n",
    "# print(extended_graph_nodes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN extended graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNSimple(nn.Module):\n",
    "    \"\"\"\n",
    "    Create a graph based on k-nearest neighbors using PyTorch.\n",
    "    Parameters:\n",
    "    - nodes: Tensor of shape (n, 3) representing 3D nodes.\n",
    "    - k: Number of nearest neighbors.\n",
    "    Returns:\n",
    "    - adjacency_matrix: Binary adjacency matrix representing the graph.\n",
    "    \"\"\"\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, nodes):\n",
    "    \n",
    "        expanded_x1 = nodes.unsqueeze(1)\n",
    "        expanded_x2 = nodes.unsqueeze(0)\n",
    "        distances = torch.norm(expanded_x1 - expanded_x2, dim=2)        # distance matrix\n",
    "\n",
    "        _, indices = torch.topk(distances, self.k + 1, largest=False, sorted=True, dim=1)\n",
    "        indices = indices[:, 1:]  # Exclude the node itself\n",
    "\n",
    "        # Create adjacency matrix\n",
    "        adjacency_matrix = torch.zeros(nodes.shape[0], nodes.shape[0], dtype=torch.float32)\n",
    "        adjacency_matrix.scatter_(1, indices, 1)\n",
    "\n",
    "        return adjacency_matrix\n",
    "\n",
    "# extended_graph_adjacency_matrix = KNNSimple(k=15)(extended_graph_nodes)\n",
    "# print(extended_graph_adjacency_matrix.shape)\n",
    "# transformation.print_graph_properties(graph=nx.from_numpy_array(extended_graph_adjacency_matrix.numpy()), display_graph=False, display_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devconv = DevConv(extended_graph_nodes,extended_graph_adjacency_matrix, 64)\n",
    "# inclusion_score_edge = devconv(previous_inclusion_score=torch.empty((0)), return_flatten=False)\n",
    "# inclusion_score_edge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAttentionEdgePredictorLayer(nn.Module):\n",
    "    def __init__(self, nodes, neighbors, size=64):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.nodes = nodes\n",
    "        self.neighbors = neighbors\n",
    "        self.wq = nn.Parameter(torch.Tensor(size))\n",
    "        self.wk = nn.Parameter(torch.Tensor(size))\n",
    "\n",
    "        nn.init.normal_(self.wq)\n",
    "        nn.init.normal_(self.wk)\n",
    "\n",
    "    def forward(self, f):\n",
    "        wq_f = self.wq.reshape(-1, 1) * f                   # Wq*f\n",
    "        wk_f = self.wk.reshape(-1, 1) * f                   # Wq*f\n",
    "        S = torch.exp(torch.matmul(wq_f.T, wk_f))           # e^((wq_f.T)*(wk_f))\n",
    "        \n",
    "        nonzero_neigh = self.neighbors.nonzero()                                                    # Find indexes of neighbors in graph\n",
    "        unique_first_elements, counts = torch.unique(nonzero_neigh[:, 0], return_counts=True)       # Count number of neighbors per node\n",
    "        split_tensors = list(torch.split(nonzero_neigh, tuple(counts)))                             # split indexes of neighbors into a list (1 element = 1 tensor of indexes)\n",
    "\n",
    "        temp = [[S[n[i,0], n[i,1]] for i in range(len(n))] for n in split_tensors]                  # For each node, get the S value for the neighbors indexes\n",
    "        summed = torch.Tensor([torch.sum(torch.Tensor(e)) for e in temp])                           # Sum these results for each nodes\n",
    "        division = summed.unsqueeze(0).repeat(1, S.shape[1], 1)[0]                                  # Repeat the sum in S.shape[1] array => division per columns\n",
    "        final_term  = S / division\n",
    "\n",
    "        return final_term\n",
    "\n",
    "\n",
    "# f = torch.mean(inclusion_score_edge, dim=1)                            # Flatten the matrix of inclusion score\n",
    "# layer = SparseAttentionEdgePredictorLayer(extended_graph_nodes, extended_graph_adjacency_matrix)\n",
    "# S = layer.forward(f)\n",
    "# print(S.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = S*np.random.choice([0, 1], size=S.shape)      # Add a random mask to emulate the 'sparse'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceCandidatesLayer(nn.Module):\n",
    "    def __init__(self, adjacency_matrix):\n",
    "        super().__init__()\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "\n",
    "    def forward(self, S):\n",
    "        A_s = torch.matmul(torch.matmul(S, self.adjacency_matrix), S.T)     # A_s = S * A * S.T\n",
    "        A_s = A_s/A_s.max()                                                 # Normalize\n",
    "        return A_s\n",
    "\n",
    "\n",
    "# layer = FaceCandidatesLayer(extended_graph_adjacency_matrix)\n",
    "# A_s = layer(torch.Tensor(S))\n",
    "# print(A_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TriConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriangleIndexes(nn.Module):\n",
    "    def __init__(self, adjacency_matrix):\n",
    "        super().__init__()\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "\n",
    "    def forward(self):\n",
    "        # tensor of indexes of each neighbors of each nodes \n",
    "        nonzero = self.adjacency_matrix.nonzero()\n",
    "        neighbors_one_indexes = nonzero.reshape(self.adjacency_matrix.shape[0],15,2)[:,:,1].clone()\n",
    "        neighbors_two_indexes = neighbors_one_indexes[neighbors_one_indexes]        # Tensor for each 2 neighbors for each nodes (neighbors of neighbors)\n",
    "        neighbors_three_indexes = neighbors_one_indexes[neighbors_two_indexes]      # Tensor for each 3 neighbors for each nodes (neighbors of neighbors of neighbors)\n",
    "\n",
    "        # Find the indices where the current index is present along the last dimension => where start node = final node (= cycle)\n",
    "        values_index_reshape = torch.arange(neighbors_three_indexes.shape[0]).repeat((15,15,15,1)).T\n",
    "        indices = (neighbors_three_indexes == values_index_reshape).nonzero()\n",
    "\n",
    "        i, j, k, l = indices[:,0], indices[:,1], indices[:,2], indices[:,3]         # First node index, Second node index, third node index, Fourth node index\n",
    "        temp_j = neighbors_one_indexes[i,j]                                         # number of the nodes firsts neighbors\n",
    "        temp_k = neighbors_two_indexes[i,j,k]                                       # number of the nodes seconds neighbors\n",
    "        temp_l = neighbors_three_indexes[i,j,k,l]                                   # number of the nodes thirds neighbors\n",
    "        triangles_indexes_test = torch.stack((i, temp_j, temp_k, temp_l), dim=1)    # nodes for each path \n",
    "        triangles_indexes_test = triangles_indexes_test[:,:3]                       # remove virtual 4th point (same as the first one (cycle))\n",
    "\n",
    "\n",
    "        # filter triangles indexes to clean the clones (=> divide the number of triangles by 6)\n",
    "        sorted_tensor, _ = torch.sort(triangles_indexes_test, dim=-1)\n",
    "        triangles_ids_igraph = torch.unique(sorted_tensor, dim=0)\n",
    "\n",
    "        return triangles_ids_igraph\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# layer_find_triangles_indexes = TriangleIndexes(extended_graph_adjacency_matrix)\n",
    "# triangles_ids_igraph = layer_find_triangles_indexes()\n",
    "# print(triangles_ids_igraph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriangleNodes(nn.Module):\n",
    "    def __init__(self, nodes):\n",
    "        super().__init__()\n",
    "        self.nodes = nodes\n",
    "\n",
    "    def forward(self, triangles_indexes):\n",
    "        return self.nodes[triangles_indexes]\n",
    "\n",
    "# layer_get_triangles = TriangleNodes(extended_graph_nodes)\n",
    "# triangles = layer_get_triangles(triangles_ids_igraph)\n",
    "# print(triangles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstPInitLayer(nn.Module):\n",
    "    def __init__(self, A_s, triangles):\n",
    "        super().__init__()\n",
    "        self.A_s = A_s\n",
    "        self.triangles = triangles\n",
    "\n",
    "    def forward(self, triangles_indexes):\n",
    "        # Extract indices for each triangle\n",
    "        i, j, k = triangles_indexes.T\n",
    "\n",
    "        # Extract probabilities using advanced indexing\n",
    "        A_s_ij = self.A_s[i, j]\n",
    "        A_s_ik = self.A_s[i, k]\n",
    "        A_s_jk = self.A_s[j, k]\n",
    "\n",
    "        # Calculate the barycenter probabilities\n",
    "        p_init = torch.zeros(self.triangles.shape[0])\n",
    "        p_init = (A_s_ij + A_s_ik + A_s_jk) / 3\n",
    "        return p_init\n",
    "\n",
    "# p_init_layer = FirstPInitLayer()\n",
    "# p_init = p_init_layer(triangles_ids_igraph)\n",
    "# print(p_init.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate barycenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarycentersLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, triangles):\n",
    "        return triangles.mean(1)\n",
    "\n",
    "# barycenters_layer = BarycentersLayer()\n",
    "# barycenters = barycenters_layer(triangles)\n",
    "# print(barycenters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(nn.Module):\n",
    "    def __init__(self, barycenters):\n",
    "        super().__init__()\n",
    "        self.barycenters = barycenters\n",
    "\n",
    "    def forward(self, x, k=20, batch_size=100):\n",
    "        indices_knn = torch.empty(size=(x.shape[0], k))\n",
    "        \n",
    "        modulo = x.shape[0]%batch_size\n",
    "        nb_iter = int((x.shape[0] - modulo) / batch_size)\n",
    "\n",
    "        for i in range(nb_iter):\n",
    "            i_start, i_end = i*batch_size, (i+1)*batch_size\n",
    "            distances = torch.norm(self.barycenters[i_start:i_end].unsqueeze(1) - self.barycenters.unsqueeze(0), dim=2)\n",
    "\n",
    "            neighbors = distances.topk(k, dim=1, largest=False).indices.clone()  # Indices of the k-nearest neighbors\n",
    "            indices_knn[i_start:i_end] = neighbors\n",
    "\n",
    "        # last piece of computation\n",
    "        distances = torch.norm(self.barycenters[-modulo:].unsqueeze(1) - self.barycenters.unsqueeze(0), dim=2)\n",
    "        neighbors = distances.topk(k, dim=1, largest=False).indices.clone()  # Indices of the k-nearest neighbors\n",
    "        indices_knn[-modulo:] = neighbors\n",
    "\n",
    "\n",
    "        return indices_knn\n",
    "\n",
    "# knn_layer = KNN()\n",
    "# indices_neigh_tri = knn_layer(barycenters).int()  #change datatype\n",
    "# print(indices_neigh_tri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMatrix(nn.Module):\n",
    "    def __init__(self, triangles, barycenters, indices_neigh_tri, number_neigh_tri):\n",
    "        super().__init__()\n",
    "        self.triangles = triangles\n",
    "        self.barycenters = barycenters\n",
    "        self.indices_neigh_tri = indices_neigh_tri\n",
    "        self.number_neigh_tri = number_neigh_tri\n",
    "\n",
    "    def forward(self):\n",
    "        # DIFF BARYCENTERS\n",
    "        barycenters_diff = np.subtract(self.barycenters[self.indices_neigh_tri[:, 0]][:, np.newaxis], self.barycenters[self.indices_neigh_tri[:, 1:]])   #Inverser la diffÃ©rence des barycentres si nÃ©cÃ©ssaire\n",
    "\n",
    "\n",
    "        # TRIANGLE EDGES NORM\n",
    "        v0, v1, v2 = self.triangles[:, 0], self.triangles[:, 1], self.triangles[:, 2]\n",
    "\n",
    "        # Calculate edge vectors\n",
    "        e_ij = torch.norm(v0 - v1, dim=1)\n",
    "        e_ik = torch.norm(v0 - v2, dim=1)\n",
    "        e_jk = torch.norm(v1 - v2, dim=1)\n",
    "\n",
    "        # Stack the edge vectors along the last dimension\n",
    "        diff_vectors = torch.stack([e_ij, e_ik, e_jk], dim=1)\n",
    "\n",
    "\n",
    "        # MAX/MIN DIFF VECTORS\n",
    "        max_diff_vectors = diff_vectors.max(dim=1).values       # calculate t_n_max\n",
    "        min_diff_vectors = diff_vectors.min(dim=1).values       # calculate t_n_min\n",
    "\n",
    "        max_diff_vectors_diff = max_diff_vectors[self.indices_neigh_tri[:, 0]][:, None] - max_diff_vectors[self.indices_neigh_tri[:, 1:]]   #Inverser la diffÃ©rence des barycentres si nÃ©cÃ©ssaire   # calculate t_n_max - t_m_max\n",
    "        min_diff_vectors_diff = min_diff_vectors[self.indices_neigh_tri[:, 0]][:, None] - min_diff_vectors[self.indices_neigh_tri[:, 1:]]   #Inverser la diffÃ©rence des barycentres si nÃ©cÃ©ssaire   # calculate t_n_min - t_m_min\n",
    "\n",
    "\n",
    "        # R MATRIX COMPUTATION\n",
    "        r_matrix = torch.zeros((self.triangles.shape[0], self.number_neigh_tri-1, 5))\n",
    "\n",
    "        r_matrix[:, :, 0]   = min_diff_vectors_diff\n",
    "        r_matrix[:, :, 1]   = max_diff_vectors_diff\n",
    "        r_matrix[:, :, 2:5] = barycenters_diff\n",
    "        \n",
    "        return r_matrix\n",
    "\n",
    "\n",
    "# r_matrix_layer = RMatrix(triangles, barycenters, indices_neigh_tri)\n",
    "# r_matrix = r_matrix_layer()\n",
    "# r_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, r_matrix, indices_neigh_tri, hidden_size):\n",
    "    super().__init__()\n",
    "    self.r_matrix = r_matrix\n",
    "    self.indices_neigh_tri = indices_neigh_tri\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "  def forward(self, p_init):\n",
    "    neigh_all = self.indices_neigh_tri[:,1:]\n",
    "\n",
    "    # Triconv 1\n",
    "    f = p_init\n",
    "    diff_p_all = (f.repeat((neigh_all.shape[1],1)).T - f[neigh_all])\n",
    "    r_diff = torch.cat((self.r_matrix, diff_p_all.unsqueeze(-1)), dim=2)\n",
    "    \n",
    "    x = nn.Flatten()(r_diff)\n",
    "    x = nn.Linear(r_diff.shape[1]*r_diff.shape[2], self.hidden_size)(x)\n",
    "    x = nn.ReLU()(x)\n",
    "    f = nn.Linear(self.hidden_size, 1)(x).squeeze()\n",
    "\n",
    "    # Triconv 2\n",
    "    diff_p_all = (f.repeat((neigh_all.shape[1],1)).T - f[neigh_all])\n",
    "    r_diff = torch.cat((self.r_matrix, diff_p_all.unsqueeze(-1)), dim=2)\n",
    "\n",
    "    x = nn.Flatten()(r_diff)\n",
    "    x = nn.Linear(r_diff.shape[1]*r_diff.shape[2], self.hidden_size)(x)\n",
    "    x = nn.ReLU()(x)\n",
    "    f = nn.Linear(self.hidden_size, 1)(x).squeeze()\n",
    "\n",
    "    # Triconv 3\n",
    "    diff_p_all = (f.repeat((neigh_all.shape[1],1)).T - f[neigh_all])\n",
    "    r_diff = torch.cat((self.r_matrix, diff_p_all.unsqueeze(-1)), dim=2)\n",
    "\n",
    "    x = nn.Flatten()(r_diff)\n",
    "    x = nn.Linear(r_diff.shape[1]*r_diff.shape[2], self.hidden_size)(x)\n",
    "    x = nn.ReLU()(x)\n",
    "    f = nn.Linear(self.hidden_size, 1)(x).squeeze()\n",
    "\n",
    "    f_softmax = nn.Softmax()(f)\n",
    "    \n",
    "    return f_softmax\n",
    "\n",
    "# mlp = MLP(r_matrix, indices_neigh_tri, 128)\n",
    "# final_scores = mlp(p_init)\n",
    "# final_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_triangles_indexes = torch.topk(final_scores, k=user_number_triangles).indices\n",
    "# selected_triangles = triangles[selected_triangles_indexes]\n",
    "# selected_triangles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_triangles_np = selected_triangles.numpy()\n",
    "\n",
    "# simplified_final_graph = nx.Graph()\n",
    "# for index_poly, poly in enumerate(selected_triangles_np):\n",
    "#     for index_current_node in range(len(poly)):\n",
    "#         current_node = tuple(poly[index_current_node])\n",
    "#         for index_other_node in range(index_current_node+1, len(poly)):\n",
    "#             edge = current_node, tuple(poly[index_other_node])\n",
    "#             simplified_final_graph.add_edge(*edge)\n",
    "#             # if attribute do not exists\n",
    "#             if len(simplified_final_graph.nodes[current_node])==0:\n",
    "#                 simplified_final_graph.nodes[current_node]['index_triangle'] = set()\n",
    "#             simplified_final_graph.nodes[current_node]['index_triangle'].add(index_poly)\n",
    "#             if len(simplified_final_graph.nodes[tuple(poly[index_other_node])])==0:\n",
    "#                 simplified_final_graph.nodes[tuple(poly[index_other_node])]['index_triangle'] = set()\n",
    "#             simplified_final_graph.nodes[tuple(poly[index_other_node])]['index_triangle'].add(index_poly)\n",
    "            \n",
    "# transformation.print_graph_properties(graph=simplified_final_graph, display_graph=False, display_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified_final_mesh = transformation.graph_to_mesh(simplified_final_graph)\n",
    "\n",
    "# #Affichage\n",
    "# transformation.mesh_to_display_vtk(mesh_data)\n",
    "# transformation.mesh_to_display_vtk(simplified_final_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Chamfer distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_d_P_Ps(p_y, x, y):\n",
    "    \"\"\"All Tensors in input\"\"\"\n",
    "    # print(p_y.shape, x.shape, y.shape)\n",
    "\n",
    "    expanded_x1 = x.unsqueeze(1)\n",
    "    expanded_x2 = y.unsqueeze(0)\n",
    "    distances = torch.norm(expanded_x1 - expanded_x2, dim=2)        # distance matrix\n",
    "\n",
    "    min_x = distances.min(dim=1).values\n",
    "    min_y = distances.min(dim=0)\n",
    "\n",
    "    first_term = torch.sum(torch.index_select(p_y, 0, min_y.indices) * min_y.values)\n",
    "    second_term = torch.sum(min_x * p_y)\n",
    "\n",
    "    return first_term + second_term\n",
    "\n",
    "\n",
    "# d_P_Ps = torch_d_P_Ps(inclusion_score, graph_nodes, extended_graph_nodes)\n",
    "# d_P_Ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Surfaces Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d_f_S_Ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_d_f_S_Ss(p_b_hat, b_hat, b):\n",
    "    # print(p_b_hat.shape, b_hat.shape, b.shape)\n",
    "\n",
    "    expanded_x1 = b_hat.unsqueeze(1)\n",
    "    expanded_x2 = b.unsqueeze(0)\n",
    "    distances = torch.norm(expanded_x1 - expanded_x2, dim=2)\n",
    "\n",
    "    min_b = distances.min(dim=1).values\n",
    "\n",
    "    final_term = torch.sum(p_b_hat * min_b)\n",
    "\n",
    "    return final_term\n",
    "\n",
    "\n",
    "\n",
    "# igraph_g_original = igraphGraph(directed=False).from_networkx(graph)\n",
    "# triangles_ids_igraph_original = np.array(igraph_g_original.cliques(min=3, max=3))\n",
    "# triangles_original = np.array(igraph_g_original.vs['_nx_name'])[triangles_ids_igraph_original]\n",
    "# b = torch.Tensor(np.mean(triangles_original, axis=1))\n",
    "\n",
    "# b_hat = selected_triangles.mean(dim=1)\n",
    "\n",
    "# p_b_hat = final_scores[selected_triangles_indexes]\n",
    "\n",
    "# d_f_S_Ss = torch_d_f_S_Ss(p_b_hat, b_hat, b)\n",
    "# d_f_S_Ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d_r_S_Ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_d_r_S_Ss(p_x, p_y, x ,y):\n",
    "    expanded_x = x.unsqueeze(1)\n",
    "    expanded_y = y.unsqueeze(0)\n",
    "    distances = torch.norm(expanded_x - expanded_y, dim=2)\n",
    "    min_d = distances.min(dim=0).values\n",
    "    first_term = p_y * min_d\n",
    "\n",
    "\n",
    "    indices_knn = distances.topk(k=50, dim=0, largest=False).indices.T  # Indices of the k-nearest neighbors\n",
    "    knn_labels = x[indices_knn]\n",
    "    xtk = torch.reshape(knn_labels, shape=((y.shape[0])*50, 3))\n",
    "\n",
    "    expanded_xtk = xtk.unsqueeze(1)\n",
    "    distances_knn = torch.norm(expanded_xtk - expanded_y, dim=2)\n",
    "    min_knn = distances_knn.min(dim=1).values\n",
    "    min_knn_reshaped = min_knn.reshape(((y.shape[0]), 50))\n",
    "\n",
    "    ptk_time_norm = p_x[indices_knn] * min_knn_reshaped\n",
    "    factor = (1-p_y) * (1/50)\n",
    "    second_term = factor * torch.sum(ptk_time_norm, dim=1)\n",
    "\n",
    "    final_term = torch.sum(first_term + second_term)\n",
    "\n",
    "    return final_term\n",
    "\n",
    "# d_f_S_Ss = torch_d_r_S_Ss(final_scores, p_b_hat, b, b_hat)\n",
    "# d_f_S_Ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(inclusion_score, graph_nodes, extended_graph_nodes, final_scores, selected_triangles, selected_triangles_indexes, graph):\n",
    "    d_P_Ps = torch_d_P_Ps(inclusion_score, graph_nodes, extended_graph_nodes)\n",
    "\n",
    "    igraph_g_original = igraphGraph(directed=False).from_networkx(graph)\n",
    "    triangles_ids_igraph_original = np.array(igraph_g_original.cliques(min=3, max=3))\n",
    "    triangles_original = np.array(igraph_g_original.vs['_nx_name'])[triangles_ids_igraph_original]\n",
    "    b = torch.Tensor(np.mean(triangles_original, axis=1))\n",
    "\n",
    "    b_hat = selected_triangles.mean(dim=1)\n",
    "\n",
    "    p_b_hat = final_scores[selected_triangles_indexes]\n",
    "\n",
    "    d_f_S_Ss = torch_d_f_S_Ss(p_b_hat, b_hat, b)\n",
    "\n",
    "    d_f_S_Ss = torch_d_r_S_Ss(final_scores, p_b_hat, b, b_hat)\n",
    "\n",
    "    loss = d_P_Ps + d_f_S_Ss + d_f_S_Ss\n",
    "\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNSimplificationMesh(nn.Module):\n",
    "    def __init__(self, graph_nodes, graph_adjacency_matrix, number_neigh_tri):\n",
    "        super().__init__()\n",
    "        self.graph_nodes = graph_nodes\n",
    "        self.graph_adjacency_matrix = graph_adjacency_matrix\n",
    "        self.number_neigh_tri = number_neigh_tri\n",
    "\n",
    "    def forward(self, user_number_triangles):\n",
    "        # POINT SAMPLER\n",
    "        gnn = GNN_Model(self.graph_nodes, self.graph_adjacency_matrix)\n",
    "        inclusion_score = gnn(torch.empty(0))\n",
    "\n",
    "        target_number_point = min(self.graph_nodes.shape[0], user_number_triangles*3)   # number of points for the simplification\n",
    "        layer = MultinomialLayer(target_number_point, self.graph_nodes)\n",
    "        extended_graph_nodes = layer.forward(inclusion_score)\n",
    "\n",
    "        extended_graph_adjacency_matrix = KNNSimple(k=15)(extended_graph_nodes)\n",
    "\n",
    "        # EDGE PREDICTOR\n",
    "        devconv = DevConv(extended_graph_nodes,extended_graph_adjacency_matrix, 64)\n",
    "        inclusion_score_edge = devconv(previous_inclusion_score=torch.empty((0)), return_flatten=False)\n",
    "\n",
    "        f = torch.mean(inclusion_score_edge, dim=1)                            # Flatten the matrix of inclusion score\n",
    "        layer = SparseAttentionEdgePredictorLayer(extended_graph_nodes, extended_graph_adjacency_matrix)\n",
    "        S = layer.forward(f)\n",
    "\n",
    "        # FACE CANDIDATES\n",
    "        layer = FaceCandidatesLayer(extended_graph_adjacency_matrix)\n",
    "        A_s = layer(torch.Tensor(S))\n",
    "\n",
    "        # FACE CLASSIFIER\n",
    "        layer_find_triangles_indexes = TriangleIndexes(extended_graph_adjacency_matrix)\n",
    "        triangles_ids_igraph = layer_find_triangles_indexes()\n",
    "\n",
    "        layer_get_triangles = TriangleNodes(extended_graph_nodes)\n",
    "        triangles = layer_get_triangles(triangles_ids_igraph)\n",
    "\n",
    "        p_init_layer = FirstPInitLayer(A_s, triangles)\n",
    "        p_init = p_init_layer(triangles_ids_igraph)\n",
    "\n",
    "        barycenters_layer = BarycentersLayer()\n",
    "        barycenters = barycenters_layer(triangles)\n",
    "\n",
    "        knn_layer = KNN(barycenters)\n",
    "        indices_neigh_tri = knn_layer(barycenters).int()  #change datatype\n",
    "\n",
    "        r_matrix_layer = RMatrix(triangles, barycenters, indices_neigh_tri, self.number_neigh_tri)\n",
    "        r_matrix = r_matrix_layer()\n",
    "\n",
    "        mlp = MLP(r_matrix, indices_neigh_tri, 128)\n",
    "        final_scores = mlp(p_init)\n",
    "\n",
    "        selected_triangles_indexes = torch.topk(final_scores, k=user_number_triangles).indices\n",
    "        selected_triangles = triangles[selected_triangles_indexes]\n",
    "\n",
    "        return selected_triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = Transformation()\n",
    "\n",
    "number_neigh_tri = 20\n",
    "stl_file_path = \"3d_models/stl/Handle.stl\"\n",
    "mesh_data = transformation.stl_to_mesh(stl_file_path)\n",
    "graph = transformation.mesh_to_graph(mesh_data)\n",
    "\n",
    "\n",
    "if len(graph._node)<20:\n",
    "    raise Exception(\"Input mesh does not have enough vertices. (More than 20 is needed)\")\n",
    "\n",
    "graph_nodes = torch.Tensor(np.array(graph))\n",
    "graph_adjacency_matrix = torch.Tensor(nx.adjacency_matrix(graph).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Mesh: b'STLB ATF 12.9.0.99 COLOR=\\xb3\\xb3\\xb3\\xff' 3532 vertices>,\n",
       " <networkx.classes.graph.Graph at 0x19209cad330>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MeshDataset(Dataset):\n",
    "    def __init__(self, mesh_dir):\n",
    "        self.mesh_dir = mesh_dir\n",
    "        self.len = 0\n",
    "        self.filepaths = list()\n",
    "        for filename in os.listdir(mesh_dir):\n",
    "            f = os.path.join(mesh_dir, filename)\n",
    "            if os.path.isfile(f):\n",
    "                self.len += 1\n",
    "                self.filepaths.append(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mesh_path = self.filepaths[idx]\n",
    "        mesh_data = transformation.stl_to_mesh(mesh_path)\n",
    "        graph = transformation.mesh_to_graph(mesh_data)\n",
    "        return mesh_data, graph\n",
    "\n",
    "torch_dataset = MeshDataset(\"3d_models/stl/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_6560\\1992214487.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result_np = torch.stack([previous_inclusion_score, torch.tensor(list_inc_score)])\n",
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_6560\\4100317574.py:14: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3618.)\n",
      "  values_index_reshape = torch.arange(neighbors_three_indexes.shape[0]).repeat((15,15,15,1)).T\n",
      "c:\\Users\\Arthur\\miniconda3\\envs\\meshPFE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 3, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gnn_model = GNNSimplificationMesh(graph_nodes, graph_adjacency_matrix, number_neigh_tri)\n",
    "# selected_triangles = gnn_model(500)\n",
    "# selected_triangles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.parameters() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mGNNSimplificationMesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Run the training loop\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m): \n",
      "\u001b[1;31mTypeError\u001b[0m: Module.parameters() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "\n",
    "gnn_model = GNNSimplificationMesh(graph_nodes, graph_adjacency_matrix, number_neigh_tri)\n",
    "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=1e-5, weight_decay=0.99)\n",
    "\n",
    "# Run the training loop\n",
    "for epoch in range(0, 5): \n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "    current_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        inputs, targets = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = gnn_model(inputs)\n",
    "        \n",
    "        loss = total_loss(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 129\n",
      "Number of edges: 621\n"
     ]
    }
   ],
   "source": [
    "selected_triangles_np = selected_triangles.numpy()\n",
    "\n",
    "simplified_final_graph = nx.Graph()\n",
    "for index_poly, poly in enumerate(selected_triangles_np):\n",
    "    for index_current_node in range(len(poly)):\n",
    "        current_node = tuple(poly[index_current_node])\n",
    "        for index_other_node in range(index_current_node+1, len(poly)):\n",
    "            edge = current_node, tuple(poly[index_other_node])\n",
    "            simplified_final_graph.add_edge(*edge)\n",
    "            # if attribute do not exists\n",
    "            if len(simplified_final_graph.nodes[current_node])==0:\n",
    "                simplified_final_graph.nodes[current_node]['index_triangle'] = set()\n",
    "            simplified_final_graph.nodes[current_node]['index_triangle'].add(index_poly)\n",
    "            if len(simplified_final_graph.nodes[tuple(poly[index_other_node])])==0:\n",
    "                simplified_final_graph.nodes[tuple(poly[index_other_node])]['index_triangle'] = set()\n",
    "            simplified_final_graph.nodes[tuple(poly[index_other_node])]['index_triangle'].add(index_poly)\n",
    "            \n",
    "transformation.print_graph_properties(graph=simplified_final_graph, display_graph=False, display_labels=False)\n",
    "\n",
    "#Affichage\n",
    "simplified_final_mesh = transformation.graph_to_mesh(simplified_final_graph)\n",
    "transformation.mesh_to_display_vtk(mesh_data)\n",
    "transformation.mesh_to_display_vtk(simplified_final_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END - END - END - END - END - END - END - END - END - END - END - END - END - END - END - END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangle Collision Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lc_le_lo(p_t, m_c_e_o, Fs):\n",
    "    \"\"\"\n",
    "    Compute the collision loss term L_c.\n",
    "\n",
    "    Parameters:\n",
    "    - p_t: 1D numpy array containing the probabilities of each triangle (indices)\n",
    "    - m_c_t: 2D numpy array containing the number of faces penetrated by each triangle\n",
    "    - Fs: 3D numpy array representing the vertices of triangles\n",
    "\n",
    "    Returns:\n",
    "    - L_c: Collision loss term\n",
    "    \"\"\"\n",
    "    assert len(p_t) == len(m_c_e_o), \"Input arrays must have the same length\"\n",
    "\n",
    "    penalty_per_triangle = p_t * m_c_e_o\n",
    "\n",
    "    # Sum the penalties for all selected triangles\n",
    "    total_penalty = np.sum(penalty_per_triangle)\n",
    "\n",
    "    # Compute the collision loss term L_c\n",
    "    L_c_e_o = (1 / len(Fs)) * total_penalty\n",
    "\n",
    "    return L_c_e_o\n",
    "\n",
    "# Example usage:\n",
    "# Replace the arrays below with your actual data\n",
    "# p_t = selected_triangles_indexes\n",
    "# m_c_t = numpy array containing the number of faces penetrated by each triangle\n",
    "# Fs = 3D numpy array representing the vertices of triangles\n",
    "p_t = selected_triangles_indexes\n",
    "Fs = triangles  # Given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NearestNeighbors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m number_neigh_barycenters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;28mlen\u001b[39m(b_hat))\n\u001b[1;32m----> 2\u001b[0m _, indexes_neigh_selected_barycenters \u001b[38;5;241m=\u001b[39m \u001b[43mNearestNeighbors\u001b[49m(n_neighbors\u001b[38;5;241m=\u001b[39mnumber_neigh_barycenters)\u001b[38;5;241m.\u001b[39mfit(b_hat)\u001b[38;5;241m.\u001b[39mkneighbors(b_hat)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NearestNeighbors' is not defined"
     ]
    }
   ],
   "source": [
    "number_neigh_barycenters = min(50, len(b_hat))\n",
    "_, indexes_neigh_selected_barycenters = NearestNeighbors(n_neighbors=number_neigh_barycenters).fit(b_hat).kneighbors(b_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_c :  4225.18776054694\n"
     ]
    }
   ],
   "source": [
    "mc = np.zeros((500))\n",
    "\n",
    "for index_neigh_barycenters in indexes_neigh_selected_barycenters:\n",
    "    current_triangle = selected_triangles[index_neigh_barycenters[0]]\n",
    "    others_triangles = selected_triangles[index_neigh_barycenters[1:]]\n",
    "\n",
    "    lines_current_triangle = LinearRing(current_triangle)\n",
    "    polygons_others_tri = MultiPolygon([Polygon(others_triangle) for others_triangle in others_triangles]).buffer(0)    # buffer 0 to correct invalid polygons => take the exterior of the shape\n",
    "\n",
    "\n",
    "    intersection = lines_current_triangle.intersection(polygons_others_tri)\n",
    "    if intersection.is_empty:\n",
    "        continue\n",
    "    if intersection.geom_type == 'MultiLineString':\n",
    "        mc[index_neigh_barycenters[0]] = len(intersection.geoms)\n",
    "    else:\n",
    "        mc[index_neigh_barycenters[0]] += 1\n",
    "L_c = compute_lc_le_lo(p_t, mc, Fs)\n",
    "print(\"L_c : \", L_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_e :  24105.658829414708\n"
     ]
    }
   ],
   "source": [
    "me = np.zeros((500))\n",
    "\n",
    "for index_neigh_barycenters in indexes_neigh_selected_barycenters:\n",
    "    current_triangle = selected_triangles[index_neigh_barycenters[0]]\n",
    "    others_triangles = selected_triangles[index_neigh_barycenters[1:]]\n",
    "\n",
    "    lines_current_triangle = LinearRing(current_triangle)\n",
    "    lines_others_tri = MultiLineString([LineString([other_tri[0], other_tri[1], other_tri[2], other_tri[0]]) for other_tri in others_triangles])\n",
    "\n",
    "    intersection = lines_current_triangle.intersection(lines_others_tri)\n",
    "    if intersection.is_empty:\n",
    "        continue\n",
    "    if intersection.geom_type == 'MultiLineString':\n",
    "        me[index_neigh_barycenters[0]] = len(intersection.geoms)\n",
    "    else:\n",
    "        me[index_neigh_barycenters[0]] += 1\n",
    "L_e = compute_lc_le_lo(p_t, me, Fs)\n",
    "print(\"L_e : \", L_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = np.zeros((500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo - Ã©chantillonnage de 100 points Ã  partir de chaque triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def sample_points_from_triangle(t, num_points=100):\n",
    "    v1, v2, v3 = t\n",
    "    bary_coords = np.random.rand(num_points, 2)\n",
    "    sqrt_bary_coords = np.sqrt(bary_coords[:, 0])\n",
    "\n",
    "    u = sqrt_bary_coords\n",
    "    v = bary_coords[:, 1]\n",
    "\n",
    "    \"\"\"\n",
    "    La formule spÃ©cifique est dÃ©rivÃ©e de l'expression gÃ©nÃ©rale d'interpolation barycentrique \n",
    "    sommets A, B et C\n",
    "    coord barycentriques: u et v\n",
    "    coord cartÃ©siennes: x,y,z \n",
    "    \"\"\"\n",
    "    x_coords = (1 - u - v) * v1[0] + u * v2[0] + v * v3[0]\n",
    "    y_coords = (1 - u - v) * v1[1] + u * v2[1] + v * v3[1]\n",
    "    z_coords = (1 - u - v) * v1[2] + u * v2[2] + v * v3[2]\n",
    "\n",
    "    sampled_points = np.column_stack((x_coords, y_coords, z_coords))\n",
    "    return sampled_points\n",
    "\n",
    "points100 = sample_points_from_triangle(triangle, num_points=100)\n",
    "points100\n",
    "points100.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les Aires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_neigh_selected_barycenters = min(50, len(b_hat))\n",
    "def knnbar(nn):\n",
    "  _, indexes_neigh_selected_barycenters = NearestNeighbors(n_neighbors=nn).fit(b_hat).kneighbors(b_hat)\n",
    "  return _, indexes_neigh_selected_barycenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 292,  20, ..., 181, 180, 141],\n",
       "       [  1,   6, 344, ..., 397, 151, 473],\n",
       "       [  2, 186,  10, ..., 185, 184, 189],\n",
       "       ...,\n",
       "       [497, 421, 418, ..., 379, 238, 457],\n",
       "       [498, 261, 303, ..., 385,  40, 327],\n",
       "       [499, 264, 126, ..., 404, 338, 320]], dtype=int64)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, indices = knnbar(number_neigh_selected_barycenters)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PÃ©nalitÃ© pour les triangles qui se chevauchent : 0.0008337502084375521\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance as scipy_distance\n",
    "\n",
    "def calculate_triangle_area(triangle):\n",
    "    # Fonction pour calculer l'aire d'un triangle en utilisant la formule de HÃ©ron\n",
    "    side_lengths = [scipy_distance.euclidean(triangle[i], triangle[(i + 1) % 3]) for i in range(3)]\n",
    "    s = sum(side_lengths) / 2\n",
    "    return np.sqrt(s * np.prod([s - length for length in side_lengths]))\n",
    "\n",
    "def penalize_overlapping_triangles(points_list, triangles, k=50):\n",
    "    # Fonction pour pÃ©naliser les triangles qui se chevauchent\n",
    "    assignment_results = []\n",
    "    overlapping_triangles = np.zeros(len(triangles), dtype=int)  # DÃ©claration en dehors de la boucle\n",
    "\n",
    "    # ajustement de la valeur maximale de k en fonction du nombre de triangles\n",
    "    k = min(k, len(triangles))\n",
    "\n",
    "    # NearestNeighbors pour trouver les k triangles les plus proches pour chaque point\n",
    "    knn_model = NearestNeighbors(n_neighbors=k).fit(np.vstack(triangles))\n",
    "    d, indices = knn_model.kneighbors(np.array(points_list)[:, 0, :])\n",
    "\n",
    "    for i, point_list in enumerate(points_list):\n",
    "        point = point_list[0]\n",
    "\n",
    "        # Verif que les indices sont valides\n",
    "        closest_triangle_indices = indices[i, :k]\n",
    "        closest_triangle_indices = closest_triangle_indices[closest_triangle_indices < len(triangles)]\n",
    "\n",
    "        # AccÃ©der aux triangles en utilisant les indices valides\n",
    "        valid_triangle_indices = []\n",
    "        for idx in closest_triangle_indices:\n",
    "            modified_triangle = np.copy(triangles[idx])\n",
    "            modified_triangle[0] = point\n",
    "\n",
    "            area_original = calculate_triangle_area(triangles[idx])\n",
    "            area_modified = calculate_triangle_area(modified_triangle)\n",
    "\n",
    "            # VÃ©rifier si la somme des aires est proche de l'aire du triangle\n",
    "            if not np.isclose(area_original, area_modified, rtol=1e-5):\n",
    "                assignment_results.append((point, triangles[idx]))\n",
    "                valid_triangle_indices.append(idx)\n",
    "\n",
    "        # MAJ du overlapping_triangles avec les indices valides\n",
    "        overlapping_triangles[valid_triangle_indices] += 1\n",
    "\n",
    "    penalties = overlapping_triangles\n",
    "    total_penalty = np.sum(penalties)\n",
    "    Lo = (1 / len(triangles)) * total_penalty\n",
    "\n",
    "    return Lo\n",
    "\n",
    "\n",
    "L_o = penalize_overlapping_triangles([points100], triangles, k=50)\n",
    "print(\"PÃ©nalitÃ© pour les triangles qui se chevauchent :\", L_o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambda_c, lambda_e, lambda_o = 0.01, 0.01, 0.01\n",
    "\n",
    "\n",
    "Loss_L = prob_chamfer_dist + d_f_S_Ss + d_r_S_Ss + (lambda_c * L_c) + (lambda_e * L_e) + (lambda_o * L_o)\n",
    "print(Loss_L)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meshPFE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
